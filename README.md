# gdOptimization
Implementation of back-propagation through different convex optimization algorithms - gradient descent, gradient descent with momentum, Nesterov's accelerated momentum, RMSProp and ADAM - for a deep neural network
